\section{Motivation}

\todo{Add references to all the parenthesis}

The design of a software system gradually evolves over time, as the understanding of the problem domain improves and the business requirements change. The cost of change of a software system is not negligible, however it is relatively lower in comparison to other Engineering Disciplines and can be controlled by adhering to principles of good design~\cite{Gof1995}. This propensity encourages iterative approach to software development; controlled improvements, extensions and revisions of the artefacts move the system from one version to the next. 

The key to controlled evolution of a software system is to rely on rigorous testing and validation at all levels of the architecture: from testing individual units of code (Unit Testing), through testing interfaces between components (Integration Testing), to testing a completely integrated software system (System Testing). Recent trends in software development practices even encourage the tests of a functionality to be developed first (Test-Driven Development). It can be argued that such systems tend to have low-coupling, as their design needs to accommodate testing components in isolation and control over the dependencies (Mocking, Dependency Injection).

In order to ensure the correct operation of a software system over time, all the levels of testing need to be automatic and fast, in order to encourage the developers to perform them locally, following every change (Automated Testing, Continuous Integration). Adhering to those principles reduces reliance on manual testing, prevents regression errors in existing functionality (Regression Testing) and improves release time.

These principles apply well to deterministic systems; testing comes down to verifying that a set of inputs will produce a particular set of outputs, because the operation of such a system can be approximated with a deterministic-state machine. The difficulty occurs when dealing with non-deterministic systems; testing involves verifying that a particular set of inputs will produce a correct set of outputs with a statistically significant probability.

Since mid-80s, financial markets have been undergoing a phase of extensive automation of the way order flows are handled. First revolution took place with the introduction of electronic markets with an open access to the limit order book, that replaced traditional, open-outcry markets. However, since mid-2000s, a second revolution is in the making: Trading Algorithms have been replacing human traders in the process of negotiating prices and reducing market impact. Those sophisticated real-time systems, designed to replicate certain trading patterns, have a clear advantage over human traders in the amount of information they can take into account, as well as the speed at which they can take positions~\cite{Lenglet}.

Although Trading Algorithms are constantly monitored, their actions take an active part in shaping the market dynamics and sometimes can contribute to 'flash crashes': brief periods of extreme market volatility. On May 6, 2010, U.S. stock market has experienced a sudden price drop of 5\%, followed by a rapid recovery, all in the course of about 30 minutes. One subsequent analysis of that incident concluded that:
\begin{quote}
[$\ldots$] technological innovation is critical for market development. However, as markets change, appropriate safeguards must be implemented to keep pace with trading practices enabled by advances in technology~\cite{Kirilenko2011}.
\end{quote}
Overall, lack of rigorous testing of Trading Algorithms and other trading systems does not only affect the P\&L  of the owner of the technology, but may also lead to dangerous instability of the market.

Trading Algorithms are an example of a non-deterministic system that operates in a highly non-deterministic environment. Trading Algorithms are designed to respond to the situation on the market and to have a certain expectation as to the effect their actions will have on the market.

Traditionally, Trading Algorithms are backtested on historical data by rebuilding the past order flows on the limit order book. Certain characteristics can be measured (e.g. VWAP of the algorithm) and compared to corresponding benchmarks (e.g. VWAP of the Market) to evaluate whether the algorithm is doing the right thing. This approach can give certain level of confidence as to the correct operation of the trading algorithm, however it nonetheless fails to model the reactions of other market participants to the order flows coming from the algorithm~\cite{Coggins2006}.

In recent years, the Agent-Based Modelling (ABM) approach has been applied to understanding the complex phenomena observed in economic and financial systems. Agent-Based Model is a computer simulation of evolving and autonomous software decision makers (agents) that interact through a set of prescribed rules. Applying ABM models to financial markets offers possibility of studying how behaviour of individual agents affects the overall market dynamics~\cite{Sorban2008}\cite{Farmer2009}. Most widely discussed approaches to Agent-Based Artificial Stock Markets simulate discrete time, call markets and employ various techniques in order to implement traders~\cite{Jha2010}.

\todo{Work on this part it seems like there is too sudden jump back to trading algos.}
In an attempt to supplement the strategy of backtesting Trading Algorithms on historical data and address its shortcomings, in this report we propose \emph{Eugene}: a real-time, continuous trading session Agent-Based Artificial Stock Market for Validation and System Testing of Trading Algorithms. We aim to demonstrate that using simple market participants can be used for discovering a class of logical polarity programming errors and differentiating between Trading Algorithms.

We will start by formulating the research objectives and null-hypotheses, followed by a discussion of the research methodologies.




